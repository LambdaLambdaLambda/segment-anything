\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{yuan_advanced_2022}
\citation{QIAO2019104958}
\@writefile{toc}{\contentsline {title}{Is Segment Anything Model a revolution in precision agriculture?}{1}{chapter.1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{ Alberto Carraro \orcidlink  {0000-0002-9747-0978} \and Francesco Marinello }{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}}
\citation{RIEHLE2020105201}
\citation{YANG2015149}
\citation{ZHANG202282}
\citation{simonyan2015vgg}
\citation{ronneberger2015unet}
\citation{badrinarayanan2016segnet}
\citation{chen2017deeplab}
\citation{krizhevsky2012alexnet}
\citation{deng2009imagenet}
\citation{simonyan2015vgg}
\citation{he2016resnet}
\citation{ronneberger2015unet}
\citation{landman2012miccai}
\citation{menze2015miccai}
\citation{codella2019skin}
\citation{chen2017deeplab}
\citation{everingham2010pascal}
\citation{cordts2016cityscapes}
\citation{zhou2017scene}
\citation{lin2014microsoft}
\citation{redmon2016yolo}
\citation{badrinarayanan2016segnet}
\citation{brostow2009semantic}
\citation{song2015sun}
\citation{EfficientNet}
\citation{zhuang2020comprehensive}
\citation{paymode_transfer_2022}
\citation{nowakowski_crop_2021}
\citation{10.1145/3209811.3212707}
\citation{mohanty2016plantvillage}
\citation{plantclef}
\citation{plant_phenotyping_dataset}
\citation{cropseg}
\citation{crop_deeplab_dataset}
\citation{isprs_semantic_labeling}
\citation{deepglobe_land_cover}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Most popular open source deep learning models and datasets used in computer vision tasks.\relax }}{3}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:models_datasets}{{1}{3}{Most popular open source deep learning models and datasets used in computer vision tasks.\relax }{table.caption.2}{}}
\citation{2023-SegGPT}
\citation{2023-SEEM}
\citation{2023-SAM-Meta}
\citation{SA-1B_dataset}
\citation{CWFID_dataset}
\citation{attention-Nips17}
\citation{GPT-3}
\citation{ViT2020}
\citation{FourierPE-Nips20}
\citation{CLIP-ICML2021}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and methods}{4}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Segment Anything}{4}{subsection.1.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Foreground vs background.\relax }}{5}{figure.caption.3}}
\newlabel{fig:fore_vs_back}{{1}{5}{Foreground vs background.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks.\relax }}{5}{figure.caption.4}}
\newlabel{fig:sam-diagram}{{2}{5}{Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks.\relax }{figure.caption.4}{}}
\citation{CWFID_dataset}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Segmentation results of SAM based on different segmentation modes.\relax }}{6}{figure.caption.5}}
\newlabel{fig:sam-modes}{{3}{6}{Segmentation results of SAM based on different segmentation modes.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{6}{section.1.3}}
\citation{ESCA_dataset}
\citation{ViT-H}
\newlabel{fig:sam1a}{{4a}{7}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:sam1a}{{a}{7}{\relax }{figure.caption.6}{}}
\newlabel{fig:sam1b}{{4b}{7}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:sam1b}{{b}{7}{\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ground truth segmentation of foliage (hand-made mask) for the file 050\_image.png\relax }}{7}{figure.caption.6}}
\newlabel{fig:true_segmentation}{{4}{7}{Ground truth segmentation of foliage (hand-made mask) for the file 050\_image.png\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Segmentation of plant images with SAM}{7}{subsection.1.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Time spent for drawing the masks manually (ground-truth labelling).\relax }}{8}{table.caption.7}}
\newlabel{tab:image_annotation_manual}{{2}{8}{Time spent for drawing the masks manually (ground-truth labelling).\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Definitions of TP, TN, FP, and FN\relax }}{8}{table.caption.8}}
\newlabel{tab:definitions}{{3}{8}{Definitions of TP, TN, FP, and FN\relax }{table.caption.8}{}}
\newlabel{fig:sam1a}{{5a}{9}{\relax }{figure.caption.12}{}}
\newlabel{sub@fig:sam1a}{{a}{9}{\relax }{figure.caption.12}{}}
\newlabel{fig:sam1b}{{5b}{9}{\relax }{figure.caption.12}{}}
\newlabel{sub@fig:sam1b}{{b}{9}{\relax }{figure.caption.12}{}}
\newlabel{fig:sam1e}{{5c}{9}{\relax }{figure.caption.12}{}}
\newlabel{sub@fig:sam1e}{{c}{9}{\relax }{figure.caption.12}{}}
\newlabel{fig:individual_SAM_mask_and_their_union}{{\caption@xref {fig:individual_SAM_mask_and_their_union}{ on input line 645}}{9}{Segmentation of plant images with SAM}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Two masks produced by SAM in automatic mode on {\color  {red} healthy1.jpg} and the union of all masks produced for the same image.\relax }}{9}{figure.caption.12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion and Conclusion}{9}{section.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{Acknowledgements}{10}{section*.15}}
\bibstyle{splncs04}
\bibdata{ref}
\bibcite{ESCA_dataset}{1}
\bibcite{badrinarayanan2016segnet}{2}
\bibcite{brostow2009semantic}{3}
\bibcite{GPT-3}{4}
\bibcite{chen2017deeplab}{5}
\bibcite{codella2019skin}{6}
\bibcite{cordts2016cityscapes}{7}
\bibcite{deepglobe_land_cover}{8}
\bibcite{deng2009imagenet}{9}
\bibcite{ViT2020}{10}
\bibcite{everingham2010pascal}{11}
\bibcite{CWFID_dataset}{12}
\bibcite{he2016resnet}{13}
\bibcite{SA-1B_dataset}{14}
\bibcite{2023-SAM-Meta}{15}
\bibcite{ViT-H}{16}
\bibcite{krizhevsky2012alexnet}{17}
\bibcite{landman2012miccai}{18}
\bibcite{plantclef}{19}
\bibcite{lin2014microsoft}{20}
\bibcite{cropseg}{21}
\bibcite{menze2015miccai}{22}
\bibcite{mohanty2016plantvillage}{23}
\bibcite{nowakowski_crop_2021}{24}
\bibcite{paymode_transfer_2022}{25}
\bibcite{QIAO2019104958}{26}
\bibcite{CLIP-ICML2021}{27}
\bibcite{redmon2016yolo}{28}
\bibcite{isprs_semantic_labeling}{29}
\bibcite{RIEHLE2020105201}{30}
\bibcite{ronneberger2015unet}{31}
\bibcite{simonyan2015vgg}{32}
\bibcite{song2015sun}{33}
\bibcite{EfficientNet}{34}
\bibcite{FourierPE-Nips20}{35}
\bibcite{attention-Nips17}{36}
\bibcite{10.1145/3209811.3212707}{37}
\bibcite{2023-SegGPT}{38}
\bibcite{YANG2015149}{39}
\bibcite{yuan_advanced_2022}{40}
\bibcite{ZHANG202282}{41}
\bibcite{plant_phenotyping_dataset}{42}
\bibcite{crop_deeplab_dataset}{43}
\bibcite{zhou2017scene}{44}
\bibcite{zhuang2020comprehensive}{45}
\bibcite{2023-SEEM}{46}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Definitions of each similarity / dissimilarity measure used in this paper.\relax }}{15}{table.caption.9}}
\newlabel{tab:metrics_used}{{4}{15}{Definitions of each similarity / dissimilarity measure used in this paper.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Time spent to place points for SAM predictor.\relax }}{16}{table.caption.10}}
\newlabel{tab:image_annotation_on_SAM_predictor}{{5}{16}{Time spent to place points for SAM predictor.\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Image segmentation time with SAM in automatic batch mode.\relax }}{16}{table.caption.11}}
\newlabel{tab:image_segmentation_on_SAM_automatic}{{6}{16}{Image segmentation time with SAM in automatic batch mode.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Evaluation of semantic segmentation performed by SAM in predictor mode with input points.\relax }}{17}{table.caption.13}}
\newlabel{tab:segmentation_evaluation_SAM_predictor}{{7}{17}{Evaluation of semantic segmentation performed by SAM in predictor mode with input points.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Evaluation of semantic segmentation performed by SAM in fully automatic mode.\relax }}{18}{table.caption.14}}
\newlabel{tab:segmentation_evaluation_SAM_automatic}{{8}{18}{Evaluation of semantic segmentation performed by SAM in fully automatic mode.\relax }{table.caption.14}{}}
