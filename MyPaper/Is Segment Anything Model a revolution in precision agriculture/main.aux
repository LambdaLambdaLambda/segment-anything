\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{yuan_advanced_2022}
\citation{QIAO2019104958}
\@writefile{toc}{\contentsline {title}{Is Segment Anything Model a revolution in precision agriculture?}{1}{chapter.1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{ Alberto Carraro \orcidlink  {0000-0002-9747-0978} \and Francesco Marinello }{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}}
\citation{RIEHLE2020105201}
\citation{YANG2015149}
\citation{ZHANG202282}
\citation{simonyan2015vgg}
\citation{ronneberger2015unet}
\citation{badrinarayanan2016segnet}
\citation{chen2017deeplab}
\citation{krizhevsky2012alexnet}
\citation{deng2009imagenet}
\citation{simonyan2015vgg}
\citation{he2016resnet}
\citation{ronneberger2015unet}
\citation{landman2012miccai}
\citation{menze2015miccai}
\citation{codella2019skin}
\citation{chen2017deeplab}
\citation{everingham2010pascal}
\citation{cordts2016cityscapes}
\citation{zhou2017scene}
\citation{lin2014microsoft}
\citation{redmon2016yolo}
\citation{badrinarayanan2016segnet}
\citation{brostow2009semantic}
\citation{song2015sun}
\citation{EfficientNet}
\citation{zhuang2020comprehensive}
\citation{paymode_transfer_2022}
\citation{nowakowski_crop_2021}
\citation{10.1145/3209811.3212707}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Most popular open source deep learning models and datasets used in computer vision tasks.\relax }}{2}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:models_datasets}{{1}{2}{Most popular open source deep learning models and datasets used in computer vision tasks.\relax }{table.caption.2}{}}
\citation{mohanty2016plantvillage}
\citation{plantclef}
\citation{plant_phenotyping_dataset}
\citation{cropseg}
\citation{crop_deeplab_dataset}
\citation{isprs_semantic_labeling}
\citation{deepglobe_land_cover}
\citation{2023-SegGPT}
\citation{2023-SEEM}
\citation{2023-SAM-Meta}
\citation{SA-1B_dataset}
\citation{ESCA_dataset}
\citation{attention-Nips17}
\citation{GPT-3}
\citation{ViT2020}
\citation{FourierPE-Nips20}
\citation{CLIP-ICML2021}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and methods}{4}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Segment Anything}{4}{subsection.1.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Foreground vs background.\relax }}{5}{figure.caption.3}}
\newlabel{fig:fore_vs_back}{{1}{5}{Foreground vs background.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks.\relax }}{5}{figure.caption.4}}
\newlabel{fig:sam-diagram}{{2}{5}{Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks.\relax }{figure.caption.4}{}}
\citation{ronneberger2015unet}
\citation{McGlinchy2019}
\citation{WANG2021106373}
\citation{smith_segmentation_2020}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Segmentation results of SAM based on different segmentation modes.\relax }}{6}{figure.caption.5}}
\newlabel{fig:sam-modes}{{3}{6}{Segmentation results of SAM based on different segmentation modes.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}U-Net and EfficientNet}{6}{subsection.1.2.2}}
\citation{EfficientNet}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces U-net architecture\relax }}{7}{figure.caption.6}}
\newlabel{fig:unet}{{4}{7}{U-net architecture\relax }{figure.caption.6}{}}
\citation{le2022cell}
\citation{liu_automatic_2022}
\citation{Zeiss23}
\citation{liu_automatic_2022}
\citation{liu_automatic_2022}
\citation{ESCA_dataset}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces U-Net architecture with EfficientNet encoder\nobreakspace  {}\cite  {liu_automatic_2022}\relax }}{8}{figure.caption.7}}
\newlabel{fig:efficientnet}{{5}{8}{U-Net architecture with EfficientNet encoder~\cite {liu_automatic_2022}\relax }{figure.caption.7}{}}
\citation{Zeiss23}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{9}{section.1.3}}
\newlabel{fig:sam1a}{{6a}{9}{\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sam1a}{{a}{9}{\relax }{figure.caption.8}{}}
\newlabel{fig:sam1b}{{6b}{9}{\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sam1b}{{b}{9}{\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Ground truth segmentation of foliage (hand-made mask) for {\color  {red}healthy1.jpg} and {\color  {red}esca1.jpg}\relax }}{9}{figure.caption.8}}
\newlabel{fig:true_segmentation}{{6}{9}{Ground truth segmentation of foliage (hand-made mask) for {\color {red}healthy1.jpg} and {\color {red}esca1.jpg}\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Time spent for drawing the masks manually (ground-truth labelling).\relax }}{10}{table.caption.9}}
\newlabel{tab:image_annotation_on_apeer}{{2}{10}{Time spent for drawing the masks manually (ground-truth labelling).\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Definitions of TP, TN, FP, and FN\relax }}{10}{table.caption.10}}
\newlabel{tab:definitions}{{3}{10}{Definitions of TP, TN, FP, and FN\relax }{table.caption.10}{}}
\citation{ESCA_dataset}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Segmentation of vine images with SAM}{11}{subsection.1.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Time spent to identify object masks with SAM using its semi-automatic modes.\relax }}{11}{table.caption.11}}
\newlabel{tab:image_annotation_on_SAM_manual}{{4}{11}{Time spent to identify object masks with SAM using its semi-automatic modes.\relax }{table.caption.11}{}}
\citation{ViT-H}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Image segmentation time with SAM in automatic batch mode.\relax }}{12}{table.caption.12}}
\newlabel{tab:image_segmentation_on_SAM_automatic}{{5}{12}{Image segmentation time with SAM in automatic batch mode.\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Evaluation of semantic segmentation performet by SAM in automatic mode\relax }}{12}{table.caption.14}}
\newlabel{tab:segmentation_evaluation_SAM_automatic}{{6}{12}{Evaluation of semantic segmentation performet by SAM in automatic mode\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Segmentation of vine images with U-Net}{12}{subsection.1.3.2}}
\newlabel{fig:sam1a}{{7a}{13}{\relax }{figure.caption.13}{}}
\newlabel{sub@fig:sam1a}{{a}{13}{\relax }{figure.caption.13}{}}
\newlabel{fig:sam1b}{{7b}{13}{\relax }{figure.caption.13}{}}
\newlabel{sub@fig:sam1b}{{b}{13}{\relax }{figure.caption.13}{}}
\newlabel{fig:sam1e}{{7c}{13}{\relax }{figure.caption.13}{}}
\newlabel{sub@fig:sam1e}{{c}{13}{\relax }{figure.caption.13}{}}
\newlabel{fig:individual_SAM_mask_and_their_union}{{\caption@xref {fig:individual_SAM_mask_and_their_union}{ on input line 484}}{13}{Segmentation of vine images with SAM}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Two masks produced by SAM in automatic mode on {\color  {red} healthy1.jpg} and the union of all masks produced for the same image.\relax }}{13}{figure.caption.13}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Evaluation of semantic segmentation performed by SAM in bounding box mode\relax }}{13}{table.caption.15}}
\newlabel{tab:segmentation_evaluation_SAM_bbox}{{7}{13}{Evaluation of semantic segmentation performed by SAM in bounding box mode\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion and Conclusion}{13}{section.1.4}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Evaluation of semantic segmentation performed by SAM in point mode\relax }}{14}{table.caption.16}}
\newlabel{tab:segmentation_evaluation_SAM_point}{{8}{14}{Evaluation of semantic segmentation performed by SAM in point mode\relax }{table.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Segmentation produced by U-Net on {\color  {red}healthy.jpg} and {\color  {red}esca1.jpg}\relax }}{14}{figure.caption.17}}
\newlabel{fig:unet_segmentation}{{8}{14}{Segmentation produced by U-Net on {\color {red}healthy.jpg} and {\color {red}esca1.jpg}\relax }{figure.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Image segmentation time with UNet\relax }}{14}{table.caption.18}}
\newlabel{tab:image_segmentation_on_UNet_automatic}{{9}{14}{Image segmentation time with UNet\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Evaluation of semantic segmentation performed by UNet.\relax }}{15}{table.caption.19}}
\newlabel{tab:segmentation_evaluation_UNet}{{10}{15}{Evaluation of semantic segmentation performed by UNet.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Acknowledgements}{15}{section*.20}}
\bibstyle{splncs04}
\bibdata{ref}
\bibcite{ESCA_dataset}{1}
\bibcite{badrinarayanan2016segnet}{2}
\bibcite{brostow2009semantic}{3}
\bibcite{GPT-3}{4}
\bibcite{chen2017deeplab}{5}
\bibcite{codella2019skin}{6}
\bibcite{cordts2016cityscapes}{7}
\bibcite{deepglobe_land_cover}{8}
\bibcite{deng2009imagenet}{9}
\bibcite{ViT2020}{10}
\bibcite{everingham2010pascal}{11}
\bibcite{he2016resnet}{12}
\bibcite{SA-1B_dataset}{13}
\bibcite{2023-SAM-Meta}{14}
\bibcite{ViT-H}{15}
\bibcite{krizhevsky2012alexnet}{16}
\bibcite{landman2012miccai}{17}
\bibcite{le2022cell}{18}
\bibcite{plantclef}{19}
\bibcite{lin2014microsoft}{20}
\bibcite{cropseg}{21}
\bibcite{metric-reload}{22}
\bibcite{McGlinchy2019}{23}
\bibcite{menze2015miccai}{24}
\bibcite{mohanty2016plantvillage}{25}
\bibcite{nowakowski_crop_2021}{26}
\bibcite{paymode_transfer_2022}{27}
\bibcite{QIAO2019104958}{28}
\bibcite{CLIP-ICML2021}{29}
\bibcite{redmon2016yolo}{30}
\bibcite{isprs_semantic_labeling}{31}
\bibcite{RIEHLE2020105201}{32}
\bibcite{ronneberger2015unet}{33}
\bibcite{simonyan2015vgg}{34}
\bibcite{smith_segmentation_2020}{35}
\bibcite{song2015sun}{36}
\bibcite{EfficientNet}{37}
\bibcite{FourierPE-Nips20}{38}
\bibcite{attention-Nips17}{39}
\bibcite{10.1145/3209811.3212707}{40}
\bibcite{WANG2021106373}{41}
\bibcite{2023-SegGPT}{42}
\bibcite{YANG2015149}{43}
\bibcite{yuan_advanced_2022}{44}
\bibcite{Zeiss23}{45}
\bibcite{ZHANG202282}{46}
\bibcite{plant_phenotyping_dataset}{47}
\bibcite{crop_deeplab_dataset}{48}
\bibcite{zhou2017scene}{49}
\bibcite{zhuang2020comprehensive}{50}
\bibcite{2023-SEEM}{51}
